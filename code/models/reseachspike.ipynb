{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "RpNr1SGVnt1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install snntorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH6Rh9eDU5VO",
        "outputId": "b8e34bff-ced2-4912-dad1-4576f3730b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.8.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/125.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from snntorch) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from snntorch) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from snntorch) (1.25.2)\n",
            "Collecting nir (from snntorch)\n",
            "  Downloading nir-1.0.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nirtorch (from snntorch)\n",
            "  Downloading nirtorch-1.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->snntorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->snntorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nir->snntorch) (3.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->snntorch) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nir, nvidia-cusolver-cu12, nirtorch, snntorch\n",
            "Successfully installed nir-1.0.1 nirtorch-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 snntorch-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from snntorch import spikegen\n",
        "import torch"
      ],
      "metadata": {
        "id": "2PR_rgFnnrjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leaky Integrate-and-Fire (LIF) Neuron Model\n"
      ],
      "metadata": {
        "id": "IFq7hGNoL12G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Suma (weight, input) apare si in cazul neuronilor spike\n",
        "- Nu se foloseste functie de activare\n",
        "- Suma (weight, input) contribuie la potentialul de membrana - U(t)\n",
        "- Daca suma (weight, input) depaseste un threshold, neuronul emite un spike\n",
        "- Input-ul unui neuron reprezinta impulsuri scurte de electricitate\n",
        "- Apare o problema deoarece este improbabil ca neuronul sa primeasca la un moment dat mai multe spike-uri input\n",
        "- Asadar avem nevoie de o persistenta temporala. Vrem sa retinem potentialul membranei in timp\n",
        "---\n",
        "- **Formula potential membrana:** U[t] = βU[t − 1] + WX[t] − S_out[t − 1]θ *(Eq4)*\n",
        "  - U[t] = potentialul membranei la momentul de timp t\n",
        "  - β = e^−1/τ = decay rate / rata de decadere\n",
        "  - WX[t] = inmultirea dintre wights si input-urile neuronului\n",
        "  - θ = threshold-ul neuronului\n",
        "  - S_out [t] ∈ {0, 1} = output-ul neuronului\n",
        "  - S_out [t] = 1 if U [t] > θ, 0 otherwise\n",
        "  - S_out[t − 1]θ = daca neuronul a emis un spike, potentialul membranei trebuie resetat *(Eq5)*\n",
        "---\n",
        "- **Resetarea soft** = βS_out[t − 1]θ\n",
        "  - Daca se depaseste treshold-ul, nu resetam potentialul membranei la zero\n",
        "  - Ofera performante mai bune(exemplu: acuratete)\n",
        "  - Inca nu se cunoaste de ce ofera performante mai bune\n",
        "- **Resetarea hard(reset-to-zero)** = S_out[t − 1]θ\n",
        "  - Daca se depaseste threshold-ul, resetam potentialul membranei la zero\n",
        "---\n",
        "**Exemplu implementare neuron LIF**"
      ],
      "metadata": {
        "id": "-XTMziK9n69L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQq8ZolsLuHA"
      },
      "outputs": [],
      "source": [
        "def lif(X, U):\n",
        "  beta = 0.9 # set decay rate\n",
        "  W = 0.5 # learnable parameter\n",
        "  theta = 1 # set threshold\n",
        "  S = 0 # initialize output spike\n",
        "\n",
        "  U = beta * U + W * X - S * theta # iterate over one time step of Eq. 4\n",
        "  S = int(U > theta) # Eq. 5\n",
        "  return S, U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemplu neuron LIF snnTorch**"
      ],
      "metadata": {
        "id": "cPij8eMhT5xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "\n",
        "lif = snn.Leaky(beta=0.9, threshold=1) # initialize neuron\n",
        "\n",
        "nr_iterations = 10\n",
        "for i in range(nr_iterations):\n",
        "  S, U = lif(X * W, U) # Eq.4 and Eq. 5 are recurrently returned\n",
        "  print(f\"Iter {i}: S = {S}, U = {U}\")"
      ],
      "metadata": {
        "id": "yMwbfzn7T6DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 input-uri cu 5 neuroni LIF - fiecare input este legat de un singur neuron LIF**"
      ],
      "metadata": {
        "id": "oWVWOY5YmuOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "lif = snn.Leaky(beta=1.0, threshold=1) # initialize neuron\n",
        "\n",
        "X = torch.rand(5) # vector of 10 random inputs\n",
        "U = torch.zeros(5) # initialize hidden states of 10 neurons to 0 V\n",
        "\n",
        "print(\"X=\", X, end=\"\\n\\n\")\n",
        "\n",
        "nr_iterations = 0\n",
        "for i in range(nr_iterations):\n",
        "  S, U = lif(X, U) # forward-pass of leaky integrate-and-fire neuron\n",
        "  print(\"Iter:\", i)\n",
        "  print(\"S:\", S)\n",
        "  print(\"U:\", U)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSnmAmJQjvy4",
        "outputId": "770dec3d-d08c-44ac-dd6f-27d0b9fae600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X= tensor([0.8809, 0.1665, 0.1596, 0.0920, 0.5783])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input encoding\n"
      ],
      "metadata": {
        "id": "sWNpzJAMiPbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Convertirea datelor in spike-uri ce sunt trimise ca input retelei neuronale\n",
        "- Input-ul retelei neuronale nu sunt neaparat spike-uri, pot fi si valori continue\n",
        "- Exista 3 codari populare\n",
        "- 1) **Rate encoding**\n",
        "  - Input mare - multe spike-uri\n",
        "  - Input mic - putine spike-uri\n",
        "  - Intr-un inverval de x steps, input-ul mare va produce mai multe spike-uri decat un input mic\n",
        "- 2) **Latency Coded Inputs**\n",
        "  - Input mare - spike rapid\n",
        "  - Input mic - spike intarziat\n",
        "  - Numarul spike-urilor nu mai conteaza\n",
        "  - Conteaza cand apare spike-ul\n",
        "  - In comparatie cu Rate Encoding, aceasta metoda atribuie mai multa\n",
        "  importanta fiecarui spike\n",
        "  - Intr-un inteval de x steps, input-ul mare va produce spike in primul step, iar input-ul mic la urma\n",
        "- 3) **Delta Modulated Inputs**\n",
        "  - Spike-uri doar cand se produce o schimbare\n",
        "  - Daca nu apare vreo schimbare, probabilitatea ca un spike sa apara este mica\n",
        "  - Spike doar daca diferenta dintre doua input-uri din perioade consecutive de timp este mai mare decat un threshold\n",
        "- Exemplele de mai sus convertesc date normale la spike-uri\n",
        "- Este mai eficient sa obtinem spike-uri natural, fara conversie (exemplu: camera DVS capteaza schimbarile din mediu folosind delta modulation)\n",
        "- In procesul de convertire al input-ului la spike-uri, se pierde informatie\n",
        "- Daca converitrea nu se poate evita, se recomanda rate encoding\n",
        "- Ideal este ca senzorii sa captureze informatia sub forma de spike-uri, pentru a nu fi necesara conversia si compresia datelor  "
      ],
      "metadata": {
        "id": "P3YuGXtGoCCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rate encoding**"
      ],
      "metadata": {
        "id": "Qed06kKohKoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 20 # number of time steps\n",
        "\n",
        "X = torch.rand(10) # vector of 10 random inputs\n",
        "S = spikegen.rate(X, num_steps=steps)\n",
        "\n",
        "print(X.size())\n",
        "print(S.size())\n",
        "# print(X)\n",
        "# print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McYDQcEekK3H",
        "outputId": "ad5ca9a7-20dd-478b-fa46-7a6667e9dcd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([20, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Latency Coded Inputs**"
      ],
      "metadata": {
        "id": "VAYWZSjAmLQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 13 # number of time steps\n",
        "\n",
        "X = torch.rand(10) # vector of 10 random inputs\n",
        "X[0] = 0.01\n",
        "X[1] = 0.02\n",
        "S = spikegen.latency(X, num_steps=steps)\n",
        "print(X.size())\n",
        "print(S.size())\n",
        "# print(X)\n",
        "# print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGGX3mgdmL8v",
        "outputId": "f6d82321-1412-41aa-eb17-ba325792111f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([13, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Delta Modulated Inputs**"
      ],
      "metadata": {
        "id": "eADNcgGBnYUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.size())\n",
        "\n",
        "S = spikegen.delta(X, threshold=0.8) # convert X to delta modulated spikes in S\n",
        "\n",
        "print(S.size())# no change to the size; only to the elements\n",
        "print(X)\n",
        "print(S)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzgY9ex9nbDW",
        "outputId": "f23809e9-7869-46ea-a184-842899f5052d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "torch.Size([10])\n",
            "tensor([0.0100, 0.0200, 0.8423, 0.8253, 0.4189, 0.1415, 0.1781, 0.8496, 0.8847,\n",
            "        0.0609])\n",
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output decoding"
      ],
      "metadata": {
        "id": "7B-rVPkN-vzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Convertirea output-ului in informatie relevanta\n",
        "- In contextul antrenarii retelelor neuronale spike, input endocoding nu constrange ce output decoding putem folosi\n",
        "- Exista 3 decodari:\n",
        "- 1) **Rate coding**\n",
        "  - Alegem neuronul din stratul output care a generat cele mai multe spike-uri\n",
        "  - Consideram problema clasificarii cu N clase.\n",
        "  - O retea neuronala non-spike alege clasa asociata neuronului cu cel mai mare output in urma functiei de activare\n",
        "  - O retea neuronala spike alege clasa asociata neoronului cu cea mai mare frecventa de spiking\n",
        "  - Neuronul este simulat de X ori, se alege neuronul care a produs de cele mai multe spike-uri\n",
        "  - Avantaje:\n",
        "    - Toleranta de eroare: se produc multe spike-uri, asadar nu este o problema daca la un moment dat neuornul nu reuseste sa produca unul\n",
        "    - Mai multe spike-uri reprezinta mai multa invatare: absenta spike-urilor poate duce la \"dead neuron problem\"\n",
        "- 2) **Latency (or temporal) coding**\n",
        "  - Alegem neuronul care emis primul un spike\n",
        "  - Rezolva problema consumului ridicat de energie(nu este nevoie de multe spike-uri cum e in cazul rate coding)\n",
        "  - Avantaje\n",
        "    - Consum enegie: mai putine spike-uri inseamna mai putina enegie disipata in hardware. De asemenea reduce numarul de accesari din memorie, din cauza sparsitatii\n",
        "    - Viteza: timpul de reactie al omului este ~250 ms, iar rata de spiking din creier este de oridinul 10Hz, asadar o persoana poate procesa doar 2-3 spike-uri in timpul de reactie. Aceasta problema la rate coding(neuronul trebuie sa produca spike-uri intr-o perioada restrictionata de timp) poate fi adresata introducand mai multi neuroni. Aceasta solutie produce un consum ridicat de enegie. Latency codes foloseste un singur spike pentru reprezentarea informatiei.\n",
        "  - Rate-coding poate explica doar 15% din activitatea neuronilor in cortexul vizual primar(V1)\n",
        "  - Daca neuronii din creier ar folosi rate-coding, s-ar consuma cu un ordin de magnitudine mai multa enegie decat codarea temporala"
      ],
      "metadata": {
        "id": "ijK_vsij-1-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functii obiectiv"
      ],
      "metadata": {
        "id": "nbBbItbmHFkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) **Spike Rate Objective Functions**\n",
        "- Multe functii de loss pot fi folosite pentru a incuraja layer-ul de output sa produca spike-uri encodate in rate code sau temporal code\n",
        "- In general, cross entropy loss sau mean square error sunt aplicate pe numarul spike-urilor sau pe potentialul membranei neuronilor din output\n",
        "- Cross Entropy Loss\n",
        "  - Spike Count\n",
        "    - Incurajeaza ca neuronul clasei corecte sa produca mai multe spike-uri\n",
        "    - Incurajeaza ca neuronii claselor incorecte sa produca mai putine spike-uri\n",
        "  - Membrane Potential\n",
        "    - Incurajeaza ca neuronul clasei corecte sa aiba potentialul membranei ridicat, rezultand spike-uri mai regulate\n",
        "- Mean Square Error\n",
        "  - Spike Count\n",
        "    - Mean square error este aplicat pe numarul spike-urilor fiecarui neuron si numarul spike-urilor target al fiecarui neuron\n",
        "    - In practica, se foloseste o proportie a numarului de spike-uri pe o perioada de timp: clasa corecta ar trebui sa produca spike 80% din timp, in timp ce clasa incorecta 20% din timp\n",
        "  - Membrane Potential\n",
        "    - Fiecare neuron output are un potential de membrana target pentru fiecare pas. Eroarea este insumata peste timp si neuroni\n",
        "- Cu un numar suficient de mare de pasi, Spike Count este mai des folosit pentru functia de loss\n",
        "\n",
        "2) **Spike Time Objectives**\n",
        "- Mai putin utilizate\n",
        "\n"
      ],
      "metadata": {
        "id": "Uqog56rEHH1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss functions in snnTorch**"
      ],
      "metadata": {
        "id": "3yY5RLxzZSiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from snntorch import functional as SF\n",
        "\n",
        "loss_1 = SF.ce_rate_loss() # cross-entropy spike rate\n",
        "loss_2 = SF.mse_rate_loss() # mean square spike rate\n",
        "loss_3 = SF.ce_max_membrane_loss() # maximum membrane\n",
        "loss_4 = SF.mse_membrane_loss() # mean square membrane"
      ],
      "metadata": {
        "id": "DBZw4oj2ZRuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spike Time Objectives in snnTorch**"
      ],
      "metadata": {
        "id": "7Ht8nnSRD1H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from snntorch import functional as SF\n",
        "\n",
        "loss_1 = SF.ce_temporal_loss() # cross-entropy spike time\n",
        "loss_2 = SF.mse_temporal_loss() # mean square spike time\n",
        "loss_3 = SF.mse_membrane_loss() # mean square membrane - target must be latency-coded"
      ],
      "metadata": {
        "id": "ovjVHXfKD1SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning rules"
      ],
      "metadata": {
        "id": "zPuDKwefEWK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dupa ce s-a ales functia de loss, aceasta trebuie folosita pentru a actualiza parametrii\n",
        "- **Perturbation learning**\n",
        "  - Se perturba weight-urile pentru a observa schimbarea erorii\n",
        "  - Daca eroarea s-a micsorat, perturbarea este acceptata, altfel este respinsa\n",
        "  - Are rezultate daca avem un numar foarte mare de incercari, ceea ce nu este practic\n",
        "  - Dificultatea invatarii creste odata cu numarul de weight-uri\n",
        "- **Random Feedback**\n",
        "  - In backpropagation eroarea este transportata de la un strat la altul\n",
        "  - Eroarea este multiplicata cu weight-ul fiecarui strat\n",
        "  - A aratat performante similare cu backopropagation in retele simple\n",
        "  - Tot feedback-ul din backpropagare este inlocuit cu unul random\n",
        "- **Local Losses**\n",
        "  - Fiecare strat are propria functie de loss\n",
        "  - Aceasta metoda se bazeaza pe faptul ca retelel neuronale shallow sunt mai usor de antrenat\n",
        "- **Forward-Forward Error Propagation**\n",
        "  - Pasul de backpropagare este inlocuit cu un alt forward-pass, unde semnalul de input este alterat in functie de eroare"
      ],
      "metadata": {
        "id": "fI3HTL7leMdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity Regularisation"
      ],
      "metadata": {
        "id": "oNTAIBspk5mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Functiile de loss cu rate codes cresc potentialul membranei si frecventa spike-urilor, fara un prag superior\n",
        "- Regularizarea loss-ului poate fi folosita pentru a penaliza frecventa crescuta/scazuta a spike-urilor\n",
        "- Regularizarea scade varianta dar poate creste bias-ul\n",
        "- Regularizare prea mare poate duce la bias mare\n",
        "- Regularizarea poate fi aplicata pe neuroni individuali sau pe grupuri de neuroni\n",
        "- 1) **Population level regularisation**\n",
        "  - Daca se doreste o eficienta enegetica crescuta, ne intereseaza numarul total de spike-uri din toata reteaua\n",
        "  - Daca numarul total de spike-uri al ale neuronilor dintr-un start depasesc o anumita valoare, atunci se poate aplica regularizarea\n",
        "- 2) **Neuron level regularisation**\n",
        "  - Putem pune pe neuroni limita minima de spike-uri, iar daca neuronul nu produce suficiente spike-uri, sa se aplice regularizarea\n",
        "- Unul dintre cele mai populare **motive pentru care reteaua neuronala spike nu invata** este numarul scazut de spike-uri al neuronilor\n",
        "- Sunt necesare teste pentru a observa ce strat are un numar scazut de spike-uri, pentru a se aplica o regularizare\n",
        "- O solutie simpla ar fi sa scadeam threshold-ul neuronilor din stratul de risc, astfel incat acestia vor produce mai usor spike-uri"
      ],
      "metadata": {
        "id": "ZpFpOawHk5xV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Spiking Neural Networks"
      ],
      "metadata": {
        "id": "NcYCHJnTqDdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trei metode principale de a antrena retelele neuronale spike\n",
        "- Rolul metodei backpropagation este de a minimiaza loss-ul\n",
        "- Pentru a minimiza loss-ul, backpropagation calculeaza derivata loss-ului cu respect fata de fiecare parametru, aplicand regula lantului de la ultimul strat spre primul strat\n",
        "- Daca gradientul este 0, nu se actualizeaza parametrii\n",
        "- Problema metodei backpropagation aplicata retelelor neuronale spike consta in non-diferentiabilitatea spike-urilor\n",
        "- Reamintim formula de calcul al potentialului membranei U [t] = βU [t − 1] + W X[t]\n",
        "  - Presupunem ca se modifica W => se modica U\n",
        "  - Daca modificarea lui U nu produce o schimbare de spike, atunci dS/dU = 0\n",
        "- 1) **Shadow training**\n",
        "  - O retea neuronala artificiala este antrenata, apoi convertita la una spike\n",
        "  - Se elimina \"dead neuron problem\"\n",
        "  - Sanse mici ca reteaua spike sa ajunga la performanata retelei artificiale intiale\n",
        "  - Conversia este folositoare atunci cand eficienta in inferenta este mai importanta decat eficienta in antrenare\n",
        "  - Functia de activare este inlocuita de spike rate sau latency coding\n",
        "- 2) **Backpropagation Using Spike Times**\n",
        "  - Reteaua neuronala spike este antrenata folosing backpropagation\n",
        "  - O metoda de a elimina dead neuron problem este de a calcula derivata in functie de timpul spike-urilor\n",
        "  - Spike-urile sunt discotinue, timpul este continuu\n",
        "  - Asadar putem calcula derivata timpului spike-urilor cu respect fata de weight-uri\n",
        "- 3) **Backpropagation Using Spikes**\n",
        "  - In pasul forward se foloseste Heavside pe U, pentru a determina daca acel neuron produce spike\n",
        "  - In pasul de backward se inlocuieste Heavside cu Sigmoid\n",
        "  - Functia Sigmoid este continua, asadar o putem deriva\n",
        "\n",
        "- 3.1) **Surrogate Gradients**\n",
        "- Ne ajuta sa scapam de dead neuron problem\n",
        "- Formulare dead neuron problem:\n",
        "  - Potentialul membranei este sub threshold: U < 0\n",
        "    - Nu se produce spike, derivata este ∂S/∂U = 0\n",
        "  - Potentialul membranei este peste threshold: U > 0\n",
        "    - Se produce spike, dar derivata ramane ∂S/∂U = 0\n",
        "  - Potentialul memebranei este egal cu threshold-ul: U = 0\n",
        "    - Se produce spike, derivata este ∂S/∂U = oo\n",
        "- Ajuta ca eroarea sa se propage catre straturi initiale\n",
        "- Spiking-ul este necesar pentru actualizarea weight-urilor\n",
        "- Surrogate gradient bun este arctan\n",
        "- Este surrogat gradient default in ssnTorch\n",
        "- Nu se stie de ce functioneaza atat de bine\n",
        "- Surrogate gradients nu pot invata daca nu exista spike-uri\n",
        "- Asta sublinieaza o importanta distinctie intre dead neuron problem si vanishing gradient problem\n",
        "  - Un dead neuron este un neuron care nu produce spike-uri => nu contribuie la loss => weight-ul asociat nu are \"credit\"\n",
        "  - Vanishing gradients pot aparea si in ANN si in SNN. Apare deoarece weight-urile ajung foarte mici in urma inmultirii repetate cu valori sub 1\n",
        "- Surrogate gradients nu trebuie specificati explicit in snnTorch, arctan se foloseste by default\n"
      ],
      "metadata": {
        "id": "9vRxSrDJqk-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Leaky integrate-and-fire neuron and surrogate gradients in snnTorch**"
      ],
      "metadata": {
        "id": "Vf7hhxJlyB9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "lif_1 = snn.Leaky(beta=0.9, spike_grad=surrogate.fast_sigmoid())\n",
        "lif_2 = snn.Leaky(beta=0.9, spike_grad=surrogate.sigmoid())\n",
        "lif_3 = snn.Leaky(beta=0.9, spike_grad=surrogate.straight_through_estimator())\n",
        "lif_4 = snn.Leaky(beta=0.9, spike_grad=surrogate.triangular())"
      ],
      "metadata": {
        "id": "feHXeMFfqIGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Link Between Surrogate Gradients and Quantized Neural Networks"
      ],
      "metadata": {
        "id": "4vfwb8B4ym-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exista cateva metode de construire a Quantized Neural Networks\n",
        "\n",
        "1) **Post training quantization**\n",
        "  - Initial, reteaua este antrenata cu aritmetica pe floating-point\n",
        "  - Dupa antrenare, weight-urile sunt convertite la precizie mai scazuta\n",
        "  - Este usor de implementat si eficient\n",
        "  - Acuratetea poate scadea mult la anumite modele si task-uri\n",
        "\n",
        "2) **Quantization-aware training**\n",
        "  - Antrenarea se efectueaza cu cunatizare\n",
        "  - Procesul de cuantizare este non-diferentiabil, asadar este ignorat in calcularea gradientului aplicand estimatorul Hinton\n",
        "  - Este mai costisitor computational si poate necesita modificari al algoritmului de antrenare\n",
        "\n",
        "3) **Mixed-precision training**\n",
        "  - Diferite parti ale retelei neuronale foloseste diferite nivele de precizie\n",
        "  - Spre exemplu, in pasul de forward se poate folosi precizie scazuta, iar in pasul de backward, cand se actualizeaza wight-urile se poate folosi precizie ridicata\n",
        "  - Complexitate computationala scazuta si impact minim asupra acuratetei\n",
        "\n",
        "4) **Binary and ternay neural networks**\n",
        "  - Caz extrem de retea cuantizata\n",
        "  - In reteaua binara weight-urile se convertesc la -1 si 1\n",
        "  - In reteaua ternata weight-urile se convertesc la -1, 0 si 1\n",
        "  - Complexitate computationala foarte scazuta\n",
        "  - Acuratete mica sau model complex\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Multe imbunatatiri in deep learning sunt rezultatul a multor tehnici de optimizare\n",
        "\n",
        "Unele dintre aceste tehnici pot fi utilizate in retelele neuronale spike, iar unele sunt specifice doar retelelor neuronale spike\n",
        "\n",
        "Exemple de tehnici utilizate in retelele neuronale spike\n",
        "\n",
        "\n",
        "1) **The reset mechanism**\n",
        "  - Reprezinta termenul din functia spike-ului\n",
        "  - Nu este diferentiabil\n",
        "  - Este important sa nu se folosesca in calcularea gradientului deoarece degradeaza performanta\n",
        "  - Se ignora acel termen in pasul de backward\n",
        "  - snnTorch face asta automat prin apelarea functiei .detach()\n",
        "\n",
        "2) **Residual connections**\n",
        "  - Functioneaza foarte bine pentru retelele neuronale non-spike si spike\n",
        "  - Se adauga conexiuni intre straturi, sarind peste cateva straturi intermediare\n",
        "  - Sunt utilizate pentru a rezolva vanishing gradient problem si a imbunatati transmiterea informatiei in pasul de forward si backward\n",
        "  - Functioneaza bine si in retelele nuronale spike\n",
        "\n",
        "3) **Learnable decay**\n",
        "  - In loc sa tratam decay-ul neuronilor ca pe un hiperparametru, il putem face parametru invatabil\n",
        "  - Seamana mai mult cu retelele neuronale recurente\n",
        "  - Imbunatateste performanta pe seturi de date variabile in timp\n",
        "\n",
        "4) **Graded spikes**\n",
        "  - Fiecare neuron are un parametru invatabil suplimentar\n",
        "  - Activarea neuronului nu mai este constransa la 0 si 1\n",
        "  - Complexitatea nu creste foarte mult deoarece numarul de parametrii invatabili creste liniar cu numarul de neuroni\n",
        "\n",
        "5) **Learnable thresholds**\n",
        "  - Nu imbunatateste procesul de invatare\n",
        "\n",
        "6) **Pooling**\n",
        "  - In retelele neuronale binarizate se foloseste pooling inainte ca valorile activarilor sa treaca prin threshold ca sa devina valori binarizate\n",
        "  - Corespunde cu a aplica pooling pe potentialul mebranei, insa asta nu imbunatateste performanta retelei\n",
        "  - Aplicam pooling pe spke-uri\n",
        "  - Cand mai multe spike-uri apar intr-o fereastra de pooling, majoritatea castiga\n",
        "  - Se ating performante ridicate\n",
        "\n",
        "7) **Optimizer**\n",
        "  - Adam si SGD performeaza bine"
      ],
      "metadata": {
        "id": "z5tvCgIFyoNS"
      }
    }
  ]
}